{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef090c5",
   "metadata": {},
   "source": [
    "<h1>ML Final Project</h1>\n",
    "<p>Course: <b>Data Mining, Machine Learning, and Deep Learning</b></p>\n",
    "<p>Course code: <b>KAN-CDSCO1004U</b></p>\n",
    "<p>Students: <b>Elias Aslaksen (S149880), Wiktoria Lazarczyk (S149985),  Luca Ludwig (S149890), Stefano Pontello (S149883)</b></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eccd8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset available at: https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/published-archive.html\n",
    "#Library import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # filter warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%load_ext tensorboard\n",
    "!rm -rf ./logs/ # Clear any logs from previous runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5527ff48",
   "metadata": {},
   "source": [
    "## 1.1 German Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e2978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"GTSRB/Final_Training/Images\" # Setting the directory folder\n",
    "\n",
    "CATEGORIES = [] #label list\n",
    "for i in range(43): # populate label list\n",
    "    CATEGORIES.append(f\"{i:05d}\") # keep zeros before number\n",
    "\n",
    "german_dataset = []\n",
    "IMG_SIZE = 100\n",
    "\n",
    "def create_dataset_german():\n",
    "    for category in CATEGORIES: \n",
    "\n",
    "        path = os.path.join(DATADIR,category)  # create path to image folder\n",
    "        class_num = CATEGORIES.index(category)  # get the index for classification  (0 = 00000, 1 = 00001 ...)\n",
    "\n",
    "        for img in tqdm(os.listdir(path)):  # iterate over each image of the classes\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to harmonize data size\n",
    "                german_dataset.append([new_array, class_num])  # add this to our training_data\n",
    "            except Exception as e:\n",
    "                pass\n",
    "create_dataset_german()\n",
    "\n",
    "# Shuffle the dataset\n",
    "random.shuffle(german_dataset)\n",
    "\n",
    "# Create X (features) and y (labels) set \n",
    "X_ger = []\n",
    "y_ger = []\n",
    "\n",
    "for features,label in german_dataset:\n",
    "    X_ger.append(features)\n",
    "    y_ger.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf43da8",
   "metadata": {},
   "source": [
    "## 1.2 Data augmentation on German Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f76915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(image):\n",
    "    rows= image.shape[0]\n",
    "    cols = image.shape[1]\n",
    "    M_rot = cv2.getRotationMatrix2D((cols/2,rows/2),10,1) # Image rotation\n",
    "    M_trans = np.float32([[1,0,3],[0,1,6]]) # Image Translation\n",
    "    img = cv2.warpAffine(image,M_rot,(cols,rows))\n",
    "    img = cv2.warpAffine(img,M_trans,(cols,rows))\n",
    "    img = cv2.bilateralFilter(img,9,75,75) # Bilateral filtering\n",
    "    return img\n",
    "\n",
    "classes = 43\n",
    "\n",
    "X_full_ger = X_ger\n",
    "y_full_ger = y_ger\n",
    "X_aug_1 = []\n",
    "Y_aug_1 = []\n",
    "\n",
    "for i in range(0,classes):\n",
    "    class_records = np.where(y_ger==i)[0].size\n",
    "    max_records = 2500\n",
    "    if class_records != max_records:\n",
    "        ovr_sample = max_records - class_records\n",
    "        samples = X[np.where(y_ger==i)[0]]\n",
    "        X_aug = []\n",
    "        Y_aug = [i] * ovr_sample\n",
    "        \n",
    "        for x in range(ovr_sample):\n",
    "            img = samples[x % class_records]\n",
    "            trans_img = data_augment(img)\n",
    "            X_aug.append(trans_img)\n",
    "            \n",
    "        X_full_ger = np.concatenate((X_full_ger, X_aug), axis=0)\n",
    "        y_full_ger = np.concatenate((y_full_ger, Y_aug)) \n",
    "        \n",
    "        Y_aug_1 = Y_aug_1 + Y_aug\n",
    "        X_aug_1 = X_aug_1 + X_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3bdc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to numpy arrays for further manipulation\n",
    "X_full_ger = np.array(X_full_ger)\n",
    "y_full_ger = np.array(y_full_ger)\n",
    "\n",
    "# Data normalization\n",
    "X_full_ger = X_full_ger/255.0\n",
    "\n",
    "y_full_ger = np.asarray(y_full_ger).astype('float32')\n",
    "n_classes = len(np.unique(y_full_ger))\n",
    "\n",
    "Y_GER_FULL = to_categorical(y_full_ger, n_classes)\n",
    "X_GER_FULL = np.array(X_full_ger).reshape(-1, 100, 100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c6140e",
   "metadata": {},
   "source": [
    "## 2.1 Danish data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eb0509",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"DANISH_TRAFFIC_SIGNS_TEST\"\n",
    "\n",
    "#DATADIR = \"/content/drive/MyDrive/ML_Project/ML Final Project/Images\"\n",
    "\n",
    "CATEGORIES = [\"00003\", \"00013\", \"00014\", \"00017\", \"00022\", \"00024\", \"00025\", \"00028\", \"00029\", \"00038\"] #label list\n",
    "\n",
    "\n",
    "dataset_danish = []\n",
    "IMG_SIZE = 100\n",
    "\n",
    "def create_dataset_danish():\n",
    "    for category in CATEGORIES: \n",
    "\n",
    "        path = os.path.join(DATADIR,category)  # create path to image folder\n",
    "        class_num = int(category)  # get the index for classification  (0 = 00000, 1 = 00001 ...)\n",
    "\n",
    "        for img in tqdm(os.listdir(path)):  # iterate over each image of the classes\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
    "                dataset_danish.append([new_array, class_num])  # add this to our training_data\n",
    "            except Exception as e:\n",
    "                pass\n",
    "create_dataset_danish()\n",
    "\n",
    "print(len(dataset_danish))\n",
    "\n",
    "# Shuffle the dataset\n",
    "random.shuffle(dataset_danish)\n",
    "\n",
    "# Create X (features) and y (labels) set \n",
    "X_dan = []\n",
    "y_dan = []\n",
    "\n",
    "for features,label in dataset_danish:\n",
    "    X_dan.append(features)\n",
    "    y_dan.append(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47515d8e",
   "metadata": {},
   "source": [
    "## 2.2 Data augmentation on Danish Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1111ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(image):\n",
    "    rows= image.shape[0]\n",
    "    cols = image.shape[1]\n",
    "    M_rot = cv2.getRotationMatrix2D((cols/2,rows/2),10,1) # Image rotation\n",
    "    M_trans = np.float32([[1,0,3],[0,1,6]]) # Image Translation\n",
    "    img = cv2.warpAffine(image,M_rot,(cols,rows))\n",
    "    img = cv2.warpAffine(img,M_trans,(cols,rows))\n",
    "    img = cv2.bilateralFilter(img,9,75,75) # Bilateral filtering\n",
    "    return img\n",
    "\n",
    "classes = [\"00003\", \"00013\", \"00014\", \"00017\", \"00022\", \"00024\", \"00025\", \"00028\", \"00029\", \"00038\"] #label list\n",
    "\n",
    "#X_dan = X_dan\n",
    "#y_dan = y_dan\n",
    "X_aug_1 = []\n",
    "Y_aug_1 = []\n",
    "\n",
    "for class_dan in classes:\n",
    "    \n",
    "    class_records = np.where(y_dan==int(class_dan))[0].size\n",
    "    max_records = 100\n",
    "    if class_records != max_records:\n",
    "        ovr_sample = max_records - class_records\n",
    "        samples = X[np.where(y_dan==int(class_dan))[0]]\n",
    "        X_aug = []\n",
    "        Y_aug = [int(class_dan)] * ovr_sample\n",
    "        \n",
    "        for x in range(ovr_sample):\n",
    "            img = samples[x % class_records]\n",
    "            trans_img = data_augment(img)\n",
    "            X_aug.append(trans_img)\n",
    "            \n",
    "        X_dan = np.concatenate((X_dan, X_aug), axis=0)\n",
    "        y_dan = np.concatenate((y_dan, Y_aug)) \n",
    "        \n",
    "        Y_aug_1 = Y_aug_1 + Y_aug\n",
    "        X_aug_1 = X_aug_1 + X_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eeda7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to numpy arrays for further manipulation\n",
    "X_dan = np.array(X_dan)\n",
    "y_dan = np.array(y_dan)\n",
    "\n",
    "# Data normalization\n",
    "X_dan = X_dan/255.0\n",
    "\n",
    "y_dan = np.asarray(y_dan).astype('float32')\n",
    "n_classes = len(np.unique(y_dan))\n",
    "\n",
    "Y_DAN_TEST = to_categorical(y_dan, n_classes)\n",
    "X_DAN_TEST = np.array(X_dan).reshape(-1, 100, 100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2dd4cb",
   "metadata": {},
   "source": [
    "## 3 Final pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c38ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_GER_TRAIN, X_GER_TEST, Y_GER_TRAIN, Y_GER_TEST = train_test_split(X_GER_FULL, Y_GER_FULL, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf48fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the Danish (test) data\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "X_DAN_TEST, Y_DAN_TEST = unison_shuffled_copies(X_DAN_TEST, Y_DAN_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3b8c2e",
   "metadata": {},
   "source": [
    "## 4.1 AlexNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccaf78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_alexnet = Sequential([\n",
    "    Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(100,100,1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "    Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "    Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(43, activation='softmax')\n",
    "])\n",
    "\n",
    "model_alexnet.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_alexnet.fit(X_ger_train,\n",
    "          y_ger_train, \n",
    "          batch_size=128, \n",
    "          epochs=10, #100\n",
    "          verbose=1,\n",
    "          validation_split=0.2\n",
    "         )\n",
    "model_alexnet.save(\"models/alexnet_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a167e0e",
   "metadata": {},
   "source": [
    "## 4.2 Tensorboard configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [2, 1]\n",
    "layer_sizes = [32, 64, 128]\n",
    "conv_layers = [2, 3, 1]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            print(NAME)\n",
    "\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Conv2D(layer_size, kernel_size=(3, 3),activation='relu', input_shape=X.shape[1:]))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            \n",
    "            for l in range(conv_layer-1):\n",
    "                model.add(Conv2D(layer_size*2, kernel_size=(3, 3), activation='relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(0.25))\n",
    "                \n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            model.add(Flatten())\n",
    "              \n",
    "            for n in range(dense_layer-1):\n",
    "                model.add(Dense(layer_size, activation='relu'))\n",
    "                model.add(Dropout(0.5))\n",
    "                \n",
    "            model.add(Dense(n_classes, activation='softmax'))\n",
    "            \n",
    "            tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME),\n",
    "                                      write_graph=True,\n",
    "                                      write_grads=True,\n",
    "                                      write_images=True)\n",
    "\n",
    "            model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            \n",
    "            model.summary()\n",
    "            model.fit(X_train,\n",
    "                      y_train, \n",
    "                      batch_size=128, \n",
    "                      epochs=10, #100\n",
    "                      verbose=1,\n",
    "                      callbacks=[tensorboard],\n",
    "                      validation_split=0.2\n",
    "                     )\n",
    "            model.save(\"models/{}\".format(NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b80cb8",
   "metadata": {},
   "source": [
    "## 5.1 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2349f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best 3 models and the Alexnet \n",
    "# NOTE: the models created with the Tensorboard architecture \n",
    "# will have unique name so the path will need to be changed \n",
    "model_3c_128n_2d = tf.keras.models.load_model('models/3-conv-128-nodes-2-dense-1653013127') \n",
    "model_3c_64n_2d = tf.keras.models.load_model('models/3-conv-64-nodes-2-dense-1653002660')\n",
    "model_3c_64n_1d = tf.keras.models.load_model('models/3-conv-64-nodes-1-dense-1653028639')\n",
    "model_alexnet = tf.keras.models.load_model('models/alexnet_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2914e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, X_test, y_test):\n",
    "    # Testing our model on Danish Test Data\n",
    "    # Convert one-hot to index\n",
    "    y_true = np.argmax(y_test, axis=1) \n",
    "    # Make prediction\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Convert one-hot to index\n",
    "    y_pred = np.argmax(y_pred, axis=1) \n",
    "    # Classification report\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.set(rc = {'figure.figsize':(18,8)})\n",
    "    ax = plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, fmt='g', ax=ax);\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels');\n",
    "    ax.set_ylabel('True labels');\n",
    "    ax.set_title('Confusion Matrix');\n",
    "    \n",
    "    print(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e690f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# German data testing \n",
    "testing(model_3c_128n_2d, X_GER_TEST, Y_GER_TEST)\n",
    "testing(model_3c_64n_2d, X_GER_TEST, Y_GER_TEST)\n",
    "testing(model_3c_64n_1d, X_GER_TEST, Y_GER_TEST)\n",
    "testing(model_alexnet, X_GER_TEST, Y_GER_TEST)\n",
    "\n",
    "# Danish data testing \n",
    "testing(model_3c_128n_2d, X_DAN_TEST, Y_DAN_TEST)\n",
    "testing(model_3c_64n_2d, X_DAN_TEST, Y_DAN_TEST)\n",
    "testing(model_3c_64n_1d, X_DAN_TEST, Y_DAN_TEST)\n",
    "testing(model_alexnet, X_DAN_TEST, Y_DAN_TEST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
